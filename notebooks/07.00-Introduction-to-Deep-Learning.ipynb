{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Deep Learning\n",
        "\n",
        "Deep learning has revolutionized the field of machine learning and artificial intelligence over the past decade.\n",
        "This chapter introduces the fundamental concepts of deep learning using modern Python tools.\n",
        "\n",
        "We'll cover:\n",
        "\n",
        "- Neural network fundamentals\n",
        "- Training deep networks\n",
        "- Convolutional Neural Networks (CNNs)\n",
        "- Recurrent Neural Networks (RNNs)\n",
        "- Transfer learning and modern architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Deep Learning?\n",
        "\n",
        "Deep learning is a subfield of machine learning based on artificial neural networks with multiple layers.\n",
        "These networks are inspired by the structure and function of the human brain, consisting of interconnected nodes that process information.\n",
        "\n",
        "Key characteristics:\n",
        "- **Multiple layers**: Networks with many hidden layers (hence \"deep\")\n",
        "- **Hierarchical feature learning**: Each layer learns increasingly complex features\n",
        "- **Automatic feature extraction**: Unlike traditional ML, features are learned automatically\n",
        "- **Scalability**: Performance often improves with more data and computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential deep learning libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Your First Neural Network\n",
        "\n",
        "Let's start with a simple neural network for classification using the classic MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Flatten the images for the simple neural network\n",
        "x_train_flat = x_train.reshape(60000, 784)\n",
        "x_test_flat = x_test.reshape(10000, 784)\n",
        "\n",
        "print(f\"Training data shape: {x_train_flat.shape}\")\n",
        "print(f\"Test data shape: {x_test_flat.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a simple neural network\n",
        "model = models.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train_flat, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test_flat, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Plot accuracy\n",
        "ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "ax1.set_title('Model Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "\n",
        "# Plot loss\n",
        "ax2.plot(history.history['loss'], label='Training Loss')\n",
        "ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
        "ax2.set_title('Model Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convolutional Neural Networks (CNNs)\n",
        "\n",
        "CNNs are specifically designed for processing grid-like data such as images. They use convolutional layers to automatically learn spatial hierarchies of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a CNN for image classification\n",
        "cnn_model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Reshape data for CNN (add channel dimension)\n",
        "x_train_cnn = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test_cnn = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Compile the CNN\n",
        "cnn_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the CNN\n",
        "cnn_history = cnn_model.fit(x_train_cnn, y_train,\n",
        "                           epochs=5,\n",
        "                           batch_size=64,\n",
        "                           validation_split=0.2,\n",
        "                           verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare performance\n",
        "cnn_test_loss, cnn_test_acc = cnn_model.evaluate(x_test_cnn, y_test, verbose=0)\n",
        "print(f\"Simple NN Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"CNN Test Accuracy: {cnn_test_acc:.4f}\")\n",
        "print(f\"Improvement: {(cnn_test_acc - test_acc) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Neural Network Activations\n",
        "\n",
        "Understanding what neural networks learn is crucial for debugging and improving them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a model to extract intermediate activations\n",
        "layer_outputs = [layer.output for layer in cnn_model.layers[:6]]\n",
        "activation_model = models.Model(inputs=cnn_model.input, outputs=layer_outputs)\n",
        "\n",
        "# Get activations for a sample image\n",
        "sample_image = x_test_cnn[0:1]\n",
        "activations = activation_model.predict(sample_image)\n",
        "\n",
        "# Visualize the activations\n",
        "layer_names = [layer.name for layer in cnn_model.layers[:6]]\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "for i, (layer_name, activation) in enumerate(zip(layer_names, activations)):\n",
        "    if len(activation.shape) == 4:  # Convolutional layers\n",
        "        # Show first few feature maps\n",
        "        for j in range(min(6, activation.shape[-1])):\n",
        "            if i * 3 + j < 6:\n",
        "                ax = axes[i // 3, i % 3] if len(axes.shape) == 2 else axes[i]\n",
        "                ax.imshow(activation[0, :, :, j], cmap='viridis')\n",
        "                ax.set_title(f'{layer_name} - Filter {j}')\n",
        "                ax.axis('off')\n",
        "                break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transfer Learning\n",
        "\n",
        "Transfer learning allows us to leverage pre-trained models for new tasks, significantly reducing training time and improving performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Using a pre-trained model (conceptual)\n",
        "# Note: This would require additional packages like tensorflow_datasets\n",
        "\n",
        "def create_transfer_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Create a transfer learning model using a pre-trained base.\n",
        "    \"\"\"\n",
        "    # Load pre-trained model (e.g., MobileNetV2)\n",
        "    base_model = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "    \n",
        "    # Freeze the base model\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    # Add custom classification head\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    return model\n",
        "\n",
        "print(\"Transfer learning model function defined.\")\n",
        "print(\"This would be used with larger image datasets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Deep Learning Concepts\n",
        "\n",
        "### 1. **Backpropagation**\n",
        "The algorithm used to train neural networks by computing gradients of the loss function with respect to each weight.\n",
        "\n",
        "### 2. **Gradient Descent**\n",
        "Optimization algorithm that iteratively adjusts weights to minimize the loss function.\n",
        "\n",
        "### 3. **Regularization**\n",
        "Techniques like dropout and L2 regularization to prevent overfitting.\n",
        "\n",
        "### 4. **Activation Functions**\n",
        "Non-linear functions that introduce complexity into the network (ReLU, sigmoid, tanh, etc.).\n",
        "\n",
        "### 5. **Loss Functions**\n",
        "Measure of how well the model is performing (cross-entropy, MSE, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for Deep Learning\n",
        "\n",
        "1. **Start Simple**: Begin with simple architectures and gradually increase complexity\n",
        "2. **Use Transfer Learning**: Leverage pre-trained models when possible\n",
        "3. **Monitor Overfitting**: Use validation sets and early stopping\n",
        "4. **Data Augmentation**: Increase dataset size through transformations\n",
        "5. **Hyperparameter Tuning**: Systematically search for optimal parameters\n",
        "6. **GPU Acceleration**: Use GPUs for faster training\n",
        "7. **Experiment Tracking**: Keep track of experiments and results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modern Deep Learning Frameworks\n",
        "\n",
        "### TensorFlow/Keras\n",
        "- Industry-standard framework\n",
        "- Excellent production deployment options\n",
        "- Strong community support\n",
        "\n",
        "### PyTorch\n",
        "- Research-friendly and flexible\n",
        "- Dynamic computation graphs\n",
        "- Growing rapidly in popularity\n",
        "\n",
        "### JAX\n",
        "- High-performance numerical computing\n",
        "- Functional programming approach\n",
        "- Excellent for research and large-scale training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Resources\n",
        "\n",
        "- [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) by François Chollet\n",
        "- [Hands-On Machine Learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032618/) by Aurélien Géron\n",
        "- [fast.ai](https://www.fast.ai/) - Practical deep learning courses\n",
        "- [Papers with Code](https://paperswithcode.com/) - Latest research and implementations\n",
        "\n",
        "This introduction provides the foundation for exploring more advanced deep learning topics in subsequent notebooks."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
