{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modern Data Science Tools\n",
        "\n",
        "This notebook introduces cutting-edge tools and libraries that are shaping modern data science workflows.\n",
        "We'll explore tools for data manipulation, visualization, machine learning, and deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Advanced Data Manipulation with Polars\n",
        "\n",
        "Polars is a fast DataFrame library implemented in Rust with a Python interface, offering significant performance improvements over pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and import polars (if not already installed)\n",
        "try:\n",
        "    import polars as pl\n",
        "    print(f\"Polars version: {pl.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Installing Polars...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polars\"])\n",
        "    import polars as pl\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Create a large dataset for comparison\n",
        "n_rows = 1_000_000\n",
        "data = {\n",
        "    'id': range(n_rows),\n",
        "    'category': np.random.choice(['A', 'B', 'C', 'D'], n_rows),\n",
        "    'value1': np.random.randn(n_rows),\n",
        "    'value2': np.random.randn(n_rows),\n",
        "    'timestamp': pl.date_range(start=pl.datetime(2023, 1, 1), end=pl.datetime(2023, 12, 31), n=n_rows)\n",
        "}\n",
        "\n",
        "# Create Polars DataFrame\n",
        "df_pl = pl.DataFrame(data)\n",
        "print(f\"Polars DataFrame shape: {df_pl.shape}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_pl.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate Polars' lazy evaluation and performance\n",
        "print(\"=== Polars Lazy API ===\")\n",
        "\n",
        "# Lazy operations - no computation yet\n",
        "lazy_df = (\n",
        "    df_pl.lazy()\n",
        "    .filter(pl.col('category') == 'A')\n",
        "    .group_by('category')\n",
        "    .agg([\n",
        "        pl.col('value1').mean().alias('mean_value1'),\n",
        "        pl.col('value2').std().alias('std_value2'),\n",
        "        pl.count().alias('count')\n",
        "    ])\n",
        "    .sort('mean_value1', descending=True)\n",
        ")\n",
        "\n",
        "# Execute the lazy query\n",
        "start_time = time.time()\n",
        "result = lazy_df.collect()\n",
        "polars_time = time.time() - start_time\n",
        "\n",
        "print(\"Result:\")\n",
        "print(result)\n",
        "print(f\"\\nPolars execution time: {polars_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Interactive Visualization with Plotly\n",
        "\n",
        "Plotly creates interactive, publication-quality visualizations that can be embedded in web applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import plotly.express as px\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    print(f\"Plotly version: {px.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Installing Plotly...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"plotly\"])\n",
        "    import plotly.express as px\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create sample data for visualization\n",
        "np.random.seed(42)\n",
        "dates = pd.date_range('2023-01-01', periods=365, freq='D')\n",
        "sales_data = pd.DataFrame({\n",
        "    'date': dates,\n",
        "    'sales': np.cumsum(np.random.randn(365) * 100 + 500),\n",
        "    'category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], 365),\n",
        "    'region': np.random.choice(['North', 'South', 'East', 'West'], 365)\n",
        "})\n",
        "\n",
        "# Interactive line plot\n",
        "fig = px.line(sales_data, x='date', y='sales', \n",
        "              title='Daily Sales Trend (Interactive)',\n",
        "              labels={'sales': 'Sales ($)', 'date': 'Date'})\n",
        "fig.update_layout(hovermode='x unified')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive scatter plot with faceting\n",
        "fig = px.scatter(sales_data, x='date', y='sales', \n",
        "                 color='category', facet_col='region',\n",
        "                 title='Sales by Category and Region',\n",
        "                 labels={'sales': 'Sales ($)', 'date': 'Date'})\n",
        "fig.update_layout(height=400)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Machine Learning with XGBoost and LightGBM\n",
        "\n",
        "Gradient boosting libraries that often outperform traditional algorithms in terms of accuracy and speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import xgboost as xgb\n",
        "    import lightgbm as lgb\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.model_selection import train_test_split, cross_val_score\n",
        "    from sklearn.metrics import mean_squared_error, r2_score\n",
        "    print(f\"XGBoost version: {xgb.__version__}\")\n",
        "    print(f\"LightGBM version: {lgb.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Installing gradient boosting libraries...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost\", \"lightgbm\"])\n",
        "    import xgboost as xgb\n",
        "    import lightgbm as lgb\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.model_selection import train_test_split, cross_val_score\n",
        "    from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create a regression dataset\n",
        "from sklearn.datasets import make_regression\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, \n",
        "                       noise=0.1, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different models\n",
        "models = {\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, eval_metric='rmse'),\n",
        "    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Train model\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
        "    \n",
        "    results[name] = {\n",
        "        'MSE': mse,\n",
        "        'R²': r2,\n",
        "        'CV R²': cv_scores.mean(),\n",
        "        'Training Time': training_time\n",
        "    }\n",
        "    \n",
        "    print(f\"  MSE: {mse:.4f}\")\n",
        "    print(f\"  R²: {r2:.4f}\")\n",
        "    print(f\"  CV R²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "    print(f\"  Training Time: {training_time:.4f}s\")\n",
        "\n",
        "# Display comparison\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "print(results_df.round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Engineering with Featuretools\n",
        "\n",
        "Automated feature engineering library that creates features from temporal and relational datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import featuretools as ft\n",
        "    print(f\"Featuretools version: {ft.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Installing Featuretools...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"featuretools\"])\n",
        "    import featuretools as ft\n",
        "\n",
        "# Create sample entity set\n",
        "es = ft.EntitySet(id=\"customer_data\")\n",
        "\n",
        "# Create customer data\n",
        "customers_df = pd.DataFrame({\n",
        "    'customer_id': range(100),\n",
        "    'age': np.random.randint(18, 80, 100),\n",
        "    'gender': np.random.choice(['M', 'F'], 100),\n",
        "    'signup_date': pd.date_range('2020-01-01', periods=100, freq='D')\n",
        "})\n",
        "\n",
        "# Create transaction data\n",
        "transactions_df = pd.DataFrame({\n",
        "    'transaction_id': range(500),\n",
        "    'customer_id': np.random.randint(0, 100, 500),\n",
        "    'amount': np.random.uniform(10, 1000, 500),\n",
        "    'transaction_date': pd.date_range('2020-01-01', periods=500, freq='6H')\n",
        "})\n",
        "\n",
        "# Add entities to entity set\n",
        "es = es.add_dataframe(\n",
        "    dataframe_name='customers',\n",
        "    dataframe=customers_df,\n",
        "    index='customer_id',\n",
        "    time_index='signup_date'\n",
        ")\n",
        "\n",
        "es = es.add_dataframe(\n",
        "    dataframe_name='transactions',\n",
        "    dataframe=transactions_df,\n",
        "    index='transaction_id',\n",
        "    time_index='transaction_date'\n",
        ")\n",
        "\n",
        "# Add relationship\n",
        "es = es.add_relationship('customers', 'customer_id', 'transactions', 'customer_id')\n",
        "\n",
        "print(\"Entity Set created:\")\n",
        "print(es)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate automated features\n",
        "feature_matrix, feature_defs = ft.dfs(\n",
        "    entityset=es,\n",
        "    target_dataframe_name='customers',\n",
        "    max_depth=2,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(f\"\\nGenerated {len(feature_defs)} features\")\n",
        "print(\"\\nFeature matrix shape:\", feature_matrix.shape)\n",
        "print(\"\\nSample features:\")\n",
        "print(feature_matrix.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Interpretation with SHAP\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) explains machine learning model predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import shap\n",
        "    print(f\"SHAP version: {shap.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Installing SHAP...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"shap\"])\n",
        "    import shap\n",
        "\n",
        "# Use the best performing model from earlier\n",
        "best_model = models['XGBoost']\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Create SHAP explainer\n",
        "explainer = shap.Explainer(best_model)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Summary plot\n",
        "print(\"SHAP Summary Plot:\")\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed explanation for a single prediction\n",
        "print(\"SHAP Waterfall Plot for Single Prediction:\")\n",
        "shap.waterfall_plot(shap_values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. MLflow for Experiment Tracking\n",
        "\n",
        "MLflow tracks experiments, reproduces runs, and deploys models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import mlflow\n",
        "    import mlflow.sklearn\n",
        "    print(f\"MLflow version: {mlflow.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Installing MLflow...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"mlflow\"])\n",
        "    import mlflow\n",
        "    import mlflow.sklearn\n",
        "\n",
        "# Set up MLflow experiment\n",
        "mlflow.set_experiment(\"Data Science Tools Comparison\")\n",
        "\n",
        "# Log an experiment\n",
        "with mlflow.start_run(run_name=\"Random_Forest_Experiment\") as run:\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
        "    mlflow.log_param(\"n_estimators\", 100)\n",
        "    mlflow.log_param(\"max_depth\", 10)\n",
        "    \n",
        "    # Train and evaluate\n",
        "    rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    \n",
        "    # Log metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    \n",
        "    mlflow.log_metric(\"mse\", mse)\n",
        "    mlflow.log_metric(\"r2_score\", r2)\n",
        "    \n",
        "    # Log the model\n",
        "    mlflow.sklearn.log_model(rf, \"random_forest_model\")\n",
        "    \n",
        "    print(f\"Experiment logged with run ID: {run.info.run_id}\")\n",
        "    print(f\"MSE: {mse:.4f}, R²: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Streamlit for Web Applications\n",
        "\n",
        "Streamlit turns data scripts into shareable web apps in minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example Streamlit app code (save as app.py to run)\n",
        "streamlit_code = '''\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "st.title(\"Data Science Tools Dashboard\")\n",
        "st.write(\"Interactive dashboard built with Streamlit\")\n",
        "\n",
        "# Sidebar controls\n",
        "st.sidebar.header(\"Controls\")\n",
        "data_size = st.sidebar.slider(\"Data Size\", 100, 1000, 500)\n",
        "noise_level = st.sidebar.slider(\"Noise Level\", 0.0, 1.0, 0.1)\n",
        "\n",
        "# Generate data\n",
        "np.random.seed(42)\n",
        "x = np.linspace(0, 10, data_size)\n",
        "y = np.sin(x) + noise_level * np.random.randn(data_size)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\"x\": x, \"y\": y})\n",
        "\n",
        "# Display data\n",
        "st.subheader(\"Generated Data\")\n",
        "st.dataframe(df.head())\n",
        "\n",
        "# Interactive plot\n",
        "st.subheader(\"Interactive Plot\")\n",
        "fig = px.line(df, x=\"x\", y=\"y\", title=\"Sine Wave with Noise\")\n",
        "st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "# Statistics\n",
        "st.subheader(\"Statistics\")\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    st.metric(\"Mean Y\", f\"{np.mean(y):.3f}\")\n",
        "    st.metric(\"Std Y\", f\"{np.std(y):.3f}\")\n",
        "with col2:\n",
        "    st.metric(\"Min Y\", f\"{np.min(y):.3f}\")\n",
        "    st.metric(\"Max Y\", f\"{np.max(y):.3f}\")\n",
        "'''\n",
        "\n",
        "print(\"Streamlit app code generated. Save this as 'app.py' and run with: streamlit run app.py\")\n",
        "print(\"\\nSample app code:\")\n",
        "print(streamlit_code[:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Modern Python Data Science Stack\n",
        "\n",
        "### Essential Libraries for 2024+:\n",
        "\n",
        "**Data Manipulation:**\n",
        "- `polars` - Fast DataFrames with lazy evaluation\n",
        "- `pandas` - Still the standard for many workflows\n",
        "- `dask` - Parallel computing with pandas-like API\n",
        "\n",
        "**Machine Learning:**\n",
        "- `scikit-learn` - Traditional ML algorithms\n",
        "- `xgboost` - Gradient boosting\n",
        "- `lightgbm` - Fast gradient boosting\n",
        "- `catboost` - Gradient boosting with categorical support\n",
        "\n",
        "**Deep Learning:**\n",
        "- `tensorflow`/`keras` - Production-ready deep learning\n",
        "- `pytorch` - Research-friendly deep learning\n",
        "- `jax` - High-performance numerical computing\n",
        "\n",
        "**Visualization:**\n",
        "- `plotly` - Interactive visualizations\n",
        "- `altair` - Declarative statistical visualization\n",
        "- `seaborn` - Statistical plots\n",
        "- `matplotlib` - Foundation plotting library\n",
        "\n",
        "**MLOps:**\n",
        "- `mlflow` - Experiment tracking\n",
        "- `dvc` - Data version control\n",
        "- `bentoml` - Model serving\n",
        "- `streamlit` - Rapid app development\n",
        "\n",
        "**Feature Engineering:**\n",
        "- `featuretools` - Automated feature engineering\n",
        "- `tsfresh` - Time series feature extraction\n",
        "- `category_encoders` - Advanced categorical encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for Modern Data Science\n",
        "\n",
        "1. **Use lazy evaluation** when possible (Polars, Dask)\n",
        "2. **Leverage GPU acceleration** for deep learning and large datasets\n",
        "3. **Track experiments** systematically with MLflow or similar tools\n",
        "4. **Automate feature engineering** to reduce manual effort\n",
        "5. **Interpret models** using SHAP or LIME for transparency\n",
        "6. **Build interactive dashboards** for stakeholder communication\n",
        "7. **Use version control** for both code and data\n",
        "8. **Containerize environments** with Docker for reproducibility\n",
        "9. **Monitor model performance** in production\n",
        "10. **Stay updated** with rapidly evolving tools and techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resources for Learning Modern Tools\n",
        "\n",
        "- [Polars Documentation](https://pola.rs/docs/)\n",
        "- [Plotly Documentation](https://plotly.com/python/)\n",
        "- [XGBoost Guide](https://xgboost.readthedocs.io/)\n",
        "- [MLflow Tracking](https://mlflow.org/docs/latest/tracking.html)\n",
        "- [Streamlit Documentation](https://docs.streamlit.io/)\n",
        "- [Featuretools Guide](https://featuretools.alteryx.com/)\n",
        "- [SHAP Documentation](https://shap.readthedocs.io/)\n",
        "\n",
        "These tools represent the cutting edge of data science and will help you build more efficient, scalable, and impactful solutions."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
