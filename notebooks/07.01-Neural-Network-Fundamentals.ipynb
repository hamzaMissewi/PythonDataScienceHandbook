{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network Fundamentals\n",
        "\n",
        "This notebook covers the mathematical foundations and practical implementation of neural networks.\n",
        "We'll build neural networks from scratch to understand how they work internally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Perceptron\n",
        "\n",
        "The perceptron is the simplest neural network unit, consisting of:\n",
        "- Inputs with weights\n",
        "- A bias term\n",
        "- An activation function\n",
        "\n",
        "Mathematically: $y = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification, make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        \n",
        "        # Training loop\n",
        "        for _ in range(self.n_iterations):\n",
        "            for i in range(n_samples):\n",
        "                # Forward pass\n",
        "                linear_output = np.dot(X[i], self.weights) + self.bias\n",
        "                y_predicted = self.activation_function(linear_output)\n",
        "                \n",
        "                # Update weights\n",
        "                update = self.learning_rate * (y[i] - y_predicted)\n",
        "                self.weights += update * X[i]\n",
        "                self.bias += update\n",
        "    \n",
        "    def activation_function(self, x):\n",
        "        return 1 if x >= 0 else 0\n",
        "    \n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return np.array([self.activation_function(x) for x in linear_output])\n",
        "\n",
        "print(\"Perceptron class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple linear classification problem\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, \n",
        "                           n_informative=2, n_clusters_per_class=1, random_state=42)\n",
        "y = y  # Convert to 0/1\n",
        "\n",
        "# Train perceptron\n",
        "perceptron = Perceptron(learning_rate=0.01, n_iterations=1000)\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "# Visualize decision boundary\n",
        "def plot_decision_boundary(X, y, model, title=\"Decision Boundary\"):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Plot data points\n",
        "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Class 0', alpha=0.7)\n",
        "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Class 1', alpha=0.7)\n",
        "    \n",
        "    # Plot decision boundary\n",
        "    if hasattr(model, 'weights'):\n",
        "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                             np.arange(y_min, y_max, 0.1))\n",
        "        \n",
        "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        \n",
        "        plt.contourf(xx, yy, Z, alpha=0.3, levels=[-1, 0, 1, 2], colors=['blue', 'red'])\n",
        "    \n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(X, y, perceptron, \"Perceptron Decision Boundary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-Layer Perceptron (MLP)\n",
        "\n",
        "A multi-layer perceptron extends the perceptron with hidden layers, enabling it to learn non-linear decision boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, layer_sizes, learning_rate=0.01, n_iterations=1000):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        \n",
        "        # Initialize weights and biases\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.1)\n",
        "            self.biases.append(np.zeros(layer_sizes[i + 1]))\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
        "    \n",
        "    def sigmoid_derivative(self, x):\n",
        "        s = self.sigmoid(x)\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "        \n",
        "        current = X\n",
        "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
        "            z = np.dot(current, w) + b\n",
        "            self.z_values.append(z)\n",
        "            \n",
        "            if i < len(self.weights) - 1:  # Hidden layers\n",
        "                current = self.sigmoid(z)\n",
        "            else:  # Output layer\n",
        "                current = z  # Linear activation for regression\n",
        "            \n",
        "            self.activations.append(current)\n",
        "        \n",
        "        return current\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        m = X.shape[0]\n",
        "        \n",
        "        # Output layer gradient\n",
        "        delta = self.activations[-1] - y.reshape(-1, 1)\n",
        "        \n",
        "        # Backpropagate through layers\n",
        "        for i in range(len(self.weights) - 1, -1, -1):\n",
        "            if i == len(self.weights) - 1:\n",
        "                dW = np.dot(self.activations[i].T, delta) / m\n",
        "                db = np.sum(delta, axis=0) / m\n",
        "            else:\n",
        "                delta = np.dot(delta, self.weights[i + 1].T) * self.sigmoid_derivative(self.z_values[i])\n",
        "                dW = np.dot(self.activations[i].T, delta) / m\n",
        "                db = np.sum(delta, axis=0) / m\n",
        "            \n",
        "            # Update weights\n",
        "            self.weights[i] -= self.learning_rate * dW\n",
        "            self.biases[i] -= self.learning_rate * db\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        for epoch in range(self.n_iterations):\n",
        "            # Forward pass\n",
        "            predictions = self.forward(X)\n",
        "            \n",
        "            # Backward pass\n",
        "            self.backward(X, y)\n",
        "            \n",
        "            # Print progress\n",
        "            if epoch % 100 == 0:\n",
        "                loss = np.mean((predictions.flatten() - y) ** 2)\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        predictions = self.forward(X)\n",
        "        return (predictions.flatten() > 0.5).astype(int)\n",
        "\n",
        "print(\"MLP class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a non-linear classification problem\n",
        "X_nonlinear, y_nonlinear = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
        "\n",
        "# Train MLP\n",
        "mlp = MLP(layer_sizes=[2, 10, 5, 1], learning_rate=0.01, n_iterations=1000)\n",
        "mlp.fit(X_nonlinear, y_nonlinear)\n",
        "\n",
        "# Visualize results\n",
        "plot_decision_boundary(X_nonlinear, y_nonlinear, mlp, \"MLP Decision Boundary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearity into neural networks. Let's explore common ones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_activation_functions():\n",
        "    x = np.linspace(-5, 5, 100)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    \n",
        "    # Sigmoid\n",
        "    sigmoid = 1 / (1 + np.exp(-x))\n",
        "    axes[0, 0].plot(x, sigmoid)\n",
        "    axes[0, 0].set_title('Sigmoid')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Tanh\n",
        "    tanh = np.tanh(x)\n",
        "    axes[0, 1].plot(x, tanh)\n",
        "    axes[0, 1].set_title('Tanh')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # ReLU\n",
        "    relu = np.maximum(0, x)\n",
        "    axes[0, 2].plot(x, relu)\n",
        "    axes[0, 2].set_title('ReLU')\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Leaky ReLU\n",
        "    leaky_relu = np.where(x > 0, x, 0.01 * x)\n",
        "    axes[1, 0].plot(x, leaky_relu)\n",
        "    axes[1, 0].set_title('Leaky ReLU')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # ELU\n",
        "    elu = np.where(x > 0, x, np.exp(x) - 1)\n",
        "    axes[1, 1].plot(x, elu)\n",
        "    axes[1, 1].set_title('ELU')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Swish\n",
        "    swish = x * (1 / (1 + np.exp(-x)))\n",
        "    axes[1, 2].plot(x, swish)\n",
        "    axes[1, 2].set_title('Swish')\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_activation_functions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient Descent Variants\n",
        "\n",
        "Different optimization algorithms for training neural networks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Optimizer:\n",
        "    def __init__(self, method='sgd', learning_rate=0.01, momentum=0.9, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.method = method\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum_buffer = {}\n",
        "        self.v_buffer = {}\n",
        "        self.m_buffer = {}\n",
        "        self.t = 0\n",
        "    \n",
        "    def update(self, params, grads, param_name):\n",
        "        self.t += 1\n",
        "        \n",
        "        if self.method == 'sgd':\n",
        "            return params - self.learning_rate * grads\n",
        "        \n",
        "        elif self.method == 'momentum':\n",
        "            if param_name not in self.momentum_buffer:\n",
        "                self.momentum_buffer[param_name] = np.zeros_like(params)\n",
        "            \n",
        "            self.momentum_buffer[param_name] = self.momentum * self.momentum_buffer[param_name] + self.learning_rate * grads\n",
        "            return params - self.momentum_buffer[param_name]\n",
        "        \n",
        "        elif self.method == 'adam':\n",
        "            if param_name not in self.v_buffer:\n",
        "                self.v_buffer[param_name] = np.zeros_like(params)\n",
        "                self.m_buffer[param_name] = np.zeros_like(params)\n",
        "            \n",
        "            self.v_buffer[param_name] = self.beta1 * self.v_buffer[param_name] + (1 - self.beta1) * grads\n",
        "            self.m_buffer[param_name] = self.beta2 * self.m_buffer[param_name] + (1 - self.beta2) * (grads ** 2)\n",
        "            \n",
        "            v_corrected = self.v_buffer[param_name] / (1 - self.beta1 ** self.t)\n",
        "            m_corrected = self.m_buffer[param_name] / (1 - self.beta2 ** self.t)\n",
        "            \n",
        "            return params - self.learning_rate * v_corrected / (np.sqrt(m_corrected) + self.epsilon)\n",
        "\n",
        "# Test optimizers on a simple quadratic function\n",
        "def quadratic_function(x, y):\n",
        "    return x**2 + y**2\n",
        "\n",
        "def quadratic_gradient(x, y):\n",
        "    return 2*x, 2*y\n",
        "\n",
        "def optimize_function(optimizer_name, n_steps=50):\n",
        "    optimizer = Optimizer(method=optimizer_name, learning_rate=0.1)\n",
        "    \n",
        "    # Starting point\n",
        "    x, y = 3.0, 2.0\n",
        "    trajectory = [(x, y)]\n",
        "    \n",
        "    for i in range(n_steps):\n",
        "        grad_x, grad_y = quadratic_gradient(x, y)\n",
        "        \n",
        "        x = optimizer.update(x, grad_x, 'x')\n",
        "        y = optimizer.update(y, grad_y, 'y')\n",
        "        \n",
        "        trajectory.append((x, y))\n",
        "    \n",
        "    return np.array(trajectory)\n",
        "\n",
        "# Compare optimizers\n",
        "optimizers = ['sgd', 'momentum', 'adam']\n",
        "colors = ['red', 'blue', 'green']\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "X, Y = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
        "Z = quadratic_function(X, Y)\n",
        "\n",
        "plt.contour(X, Y, Z, levels=20, alpha=0.3)\n",
        "\n",
        "for opt_name, color in zip(optimizers, colors):\n",
        "    trajectory = optimize_function(opt_name)\n",
        "    plt.plot(trajectory[:, 0], trajectory[:, 1], 'o-', color=color, label=opt_name, markersize=4)\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Optimization Algorithms Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Perceptrons** can only learn linear decision boundaries\n",
        "2. **Multi-layer networks** can learn complex non-linear patterns\n",
        "3. **Activation functions** introduce non-linearity and are crucial for deep networks\n",
        "4. **Backpropagation** efficiently computes gradients for training\n",
        "5. **Optimization algorithms** significantly affect training speed and convergence\n",
        "\n",
        "These fundamentals form the foundation for modern deep learning architectures."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
